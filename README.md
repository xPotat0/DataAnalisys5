# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]



Отчет по лабораторной работе #5 выполнил:
- Исмагилов Денис Рустамович
- РИ210945
Отметка о выполнении заданий (заполняется студентом):


| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |


знак "*" - задание выполнено; знак "#" - задание не выполнено;


Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.




## Цель работы
 - Интеграция экономической системы в проект Unity и обучение ML-Agent.


## Задание 1
 - Изменить параметры файла .yaml-агента и определить какие параметры и как влияют на обучение модели.

## Ход работы:
Исследуем проект, который нам предоставили.
 - В модели реализовано передвижение Агента RollerAgent между 
двумя объектами: Target и GoldMine. Модель представляет собой 
упрощенный симулятор добычи ресурсов и их транспортировки.
 - Направление движения Агента определяется переменными скрипт-файла Move.cs в инспекторе свойств:

![1](https://user-images.githubusercontent.com/106258306/204143048-8d7e58d8-5da6-43cc-8a6b-23e0a76f9247.png)

 - Модель обучения Агента, как и ранее определяется скрипт-файлами, 
импортированными из ml-agents-release_19: Decision Requester и 
Behavior Parameters. 
 - В скрипт-файле Decision Requester устанавливается период 
принятия решения, равный 10. Это значит, что цикл наблюдение-решение-действие-вознаграждение повторяется каждый раз, когда 
агент запрашивает решение. Агенты будут запрашивать решение при 
вызове Agent.RequestDecision(). Через период Decision Period = 10.
Бывают ситуации, когда нужно, чтобы агент запрашивал решения 
самостоятельно через регулярные промежутки времени. В этом 
случае мы можем использовать в коде компонент Decision Requester 
в GameObject агента.

![1](https://user-images.githubusercontent.com/106258306/204144044-8170f14d-07fa-4266-b2f7-c07c0b3f17fd.png)

 - Также обратитим внимание на настройки параметров поведения 
агента Behavior Parameters. Здесь Vector Observation определяется 
параметрами: Space Size - длина наблюдения вектора для Агента. 
Stacked Vectors - Количество предыдущих векторных наблюдений, 
которые будут суммироваться и совместно использоваться для 
принятия решений. Другими словами, Vector Observation определяет 
эффективный размер векторного наблюдения, передаваемого в 
политику, составляет: Space Size x Stacked Vectors. 

![1](https://user-images.githubusercontent.com/106258306/204144101-d2e8273b-94ad-4931-bd50-0fc4a49a6c9d.png)

 - Action (действия) определяют количество одновременных 
непрерывных действий (непрерывные действия — Continuous
Actions), которые может выполнять агент. Дискретные ветви — 
массив целых чисел, определяющий несколько одновременных 
дискретных действий. Значения в массиве Discrete Branches 
соответствуют количеству возможных дискретных значений для 
каждой ветви действия.

![1](https://user-images.githubusercontent.com/106258306/204144445-c052550f-ef0a-4dc3-8841-dc0b252c3915.png)

 - Скрипт-файл Move.cs описывает поведение при движении Агента. А 
также его вознаграждение и обучение:

|Функция в скрипт-файле Moce.cs|Пояснение работы функции|
|-|-|
|public override void CollectObservations(VectorSensor sensor)|В этой функции задаются параметры, на основе которых обучается Агент (сенсоры агента)|
|public override void OnActionReceived(ActionBuffers actionBuffers)|В этой функции описываются основные действия и взаимодействия Агента с внешним миром. А также его «вознаграждение» за успешное или не успешное обучение.В нашем примере в эту функцию входят условия передвижения агента, и расчет темпа инфляции.//стандартная функция Агента|
|IEnumerator StartGoldMine()|Coroutine – сопрограмма, которая выполняется параллельно выполняемому скрипту.В нашем случае эта функция отвечает за старт добычи «золота в шахте».|
|IEnumerator StartMonth()|Coroutine – сопрограмма, которая выполняется параллельно выполняемому скрипту.В нашем случае эта функция отвечает за отсчет месяца (фоновый счетчик).|

Посмотрим .yaml файл для конфигурации обучающего тренажера.

```yaml
behaviors:
  Economic:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 10240
      learning_rate: 3.0e-4
      learning_rate_schedule: linear
      beta: 1.0e-2
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3      
    network_settings:
      normalize: false
      hidden_units: 128
      num_layers: 2
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 500000
    max_steps: 750000
    time_horizon: 64
    summary_freq: 5000
    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10
```

Поместим файл Economic.yaml в папку с проектом Unity. В нем описываются, какой тренажер использовать: PPO или SAC (парметр trainer_type). Количество опыта, которое необходимо собрать, прежде чем создавать и отображать тренировочную статистику (параметр summary_freq, он определяет детализацию графиков в Tensorboard), и так далее.

Запустим Anaconda Prompt от имени администратора и запустим MLAgent, как делали в предыдущих работах:
 - Создадим виртуальное пространство командой
```
conda create -n MLAgent python=3.6.13
```
 - Активируем созданное пространство
```
conda activate MLAgent
```
 - Установим пакеты MLAgent и Torch
```
pip install mlagents==0.28.0
pip install torch~=1.7.1 -f https://download.pytorch.org/whl/torch_stable.html
```
 - Перейдём в папу с проектом (Где лежит Economic.yaml файл)
```
cd C:\Users\111\MLA_Lab5\MLA_Lab4
```
 - Запустим обучение ML-Агента
```
mlagents-learn Economic.yaml --run-id=Ecomonic --force
```

![1](https://user-images.githubusercontent.com/106258306/204144752-4734f6e8-d7f0-418c-ae1b-9c3cf53d1ba8.png)

 - Запустим сцену в Unity. Если всё было сделано правильно, то в командой строке появится сообщение о считывании .yaml файла.

![1](https://user-images.githubusercontent.com/106258306/204144823-df1dca77-1627-45b9-a869-3a871468cb9c.png)

- Шары на сцене начали двигаться.

Понаблюдаем за процессом обучения. В консоль может быть выведено два сообщения:
|Пример сообщения|Комментарий|
|-|-|
|[INFO] Economic. Step: 5000. Time Elapsed: 89.651 s. No episode was completed since last summary. Training.|Пример неудачной итерации. За 5000 шагов, которые прошли за 89.651 сек. обучение не произведено|
|[INFO] Economic. Step: 10000. Time Elapsed: 112.806 s. Mean Reward: 1.000. Std of Reward: 0.000. Training. ELO: 1200.006.|Пример удачной итерации. Агент частично обучился за 10000 шагов с вознаграждением 0.000.|

Завершим процесс обучения. Для этого в командной строке нажмём сочетание Ctrl+C. После этого отключим сцену в Unity.

Результаты обучения модели были сохранены в папку с .yaml файлом.

![1](https://user-images.githubusercontent.com/106258306/204145615-45a3a051-0845-40a0-9d0b-38f55102fb18.png)

Построим графики. Для этого установим TensorBoard, используя командную строку.

```
pip install tensorflow
```

После завершения установки запустим TensorBoard

```
tensorboard --logdir=results
```

По умолчанию TensorBoard запускается по ссылке:
 -  http://localhost:6006:
 
 Перейдём по ссылке и посмотрим на результаты.
 
 ![1](https://user-images.githubusercontent.com/106258306/204308664-01f3fcec-b533-4c1d-bbe2-f62cff337e4b.png)

На сайте построилось 5 графиков.

График Cumulative Reward - показывает накопительное вознаграждение к количеству шагов.

График Policy Loss - Функция потерь.

Наш график получился кривым. Попробуем изменить коэфициенты в скрипт файле Move.cs. Изменим диапозон из которого берется значение для speedMove в методе OnActionReceived:

```c#
speedMove = Mathf.Clamp(actionBuffers.ContinuousActions[0], 3f, 7f);
```

Переобучим нашу модель и посмотрим на результаты.

![1](https://user-images.githubusercontent.com/106258306/204316908-4ce1424d-2f8f-4e8c-a0d2-2616aa30ad82.png)

Можно заметить, что получаемые награды на графике Cumulative Reward стали больше, но остальные графики остались прежнее.
Вернём значения коэфициента на прежнее значение и поменяем коэфициенты во времени добычи.

```
speedMove = Mathf.Clamp(actionBuffers.ContinuousActions[0], 1f, 10f);
timeMining = Mathf.Clamp(actionBuffers.ContinuousActions[1], 3f, 7f);
```

Переобучим модель и посмотрим на результаты.

![1](https://user-images.githubusercontent.com/106258306/204319447-656a9227-ebc6-47c2-8ed3-14e69b7c7dda.png)

График Cumulative Reward застыл на отметке 1, а из график Policy Loss исчезли скачки.

Вернём значение коэфициентов времени копания и изменим коэфициенты на цене золота.

```c#
amountGold = Mathf.Clamp(actionBuffers.ContinuousActions[2], 5f, 15f);
```

![1](https://user-images.githubusercontent.com/106258306/204321482-7927efe7-eabe-4ea8-893b-d2b4121107e9.png)

График Cumulative Reward стал возрастать, а график Policy Loss стал приближаться к прямой линии.

Оставим это изменение и изменим цену на кирку.

```c#
pickaxeСost = Mathf.Clamp(actionBuffers.ContinuousActions[3], 70f, 800f);
```

![1](https://user-images.githubusercontent.com/106258306/204323107-4f479a81-1912-4086-ab43-4aabbabbd928.png)

График Cumulative Reward остался прежним, но график Policy Loss стал более кривым.

 ## Задание 2
 - Объяснение графиков TensorBoard
 
 ## Ход работы
 
 - Самым эффективным изменением стало изменение цены золота.
 - График Cumulative Reward показывает, как со временем менялось вознаграждение.
 - Гистограмма Cumulative Reward показывает, какие слои модели работали при обучении.
 - График Losses/Policy Loss показывает точность потерь награды со временем .
 - График Losses/Value Loss показывает, как изменялись с течением времени, изучила ли модель информацию. Если график "скачет", то модель ничего не выучила. Если график плавная линия, которая стремится к прямой, то модель изучила информацию.

## Вывод

Я узнал и научился пользоваться пакетом TensorFlow, который нужен для визуализации обучения MLAgent. Этот инструмент используется для анализа, как обучалась и менялась модель при обучении.
